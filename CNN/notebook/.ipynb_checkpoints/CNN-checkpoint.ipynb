{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ab1b90c",
   "metadata": {},
   "source": [
    "Author: KuoChen Huang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66a1c44",
   "metadata": {},
   "source": [
    "Reference:\n",
    "1. https://www.geeksforgeeks.org/how-to-read-multiple-text-files-from-folder-in-python/\n",
    "2. https://datagy.io/python-remove-punctuation-from-string/ \n",
    "3. https://machinelearningknowledge.ai/keras-tokenizer-tutorial-with-examples-for-fit_on_texts-texts_to_sequences-texts_to_matrix-sequences_to_matrix/\n",
    "4. https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
    "5. https://stackabuse.com/python-for-nlp-word-embeddings-for-deep-learning-in-keras/ \n",
    "6. https://ithelp.ithome.com.tw/articles/10194633\n",
    "7.https://www.projectpro.io/recipes/what-is-drop-out-rate-keras#:~:text=Dropout%20can%20be%20implemented%20by,the%20skill%20of%20the%20model. \n",
    "8. https://stackoverflow.com/questions/53838304/format-of-adding-hidden-layers-in-keras\n",
    "9. https://vimsky.com/zh-tw/examples/detail/python-method-keras.layers.pooling.MaxPooling1D.html\n",
    "10. https://keras.io/api/layers/convolution_layers/convolution1d/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdc794f",
   "metadata": {},
   "source": [
    "## TextClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01856c3",
   "metadata": {},
   "source": [
    "It is highly recommended that you complete this project using Keras and Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc02e3c5",
   "metadata": {},
   "source": [
    "### (a) In this problem, we are trying to build a classifier to analyze the sentiment of reviews. You are provided with text data in two folders: one folder involves positive reviews, and one folder involves negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e067cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import operator\n",
    "import statistics\n",
    "import math\n",
    "\n",
    "neg_path = \"../data/neg\"\n",
    "pos_path = \"../data/pos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f42a5b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read text File\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return (f.read())\n",
    "\n",
    "# iterate through all file\n",
    "def read_all_file(path):\n",
    "    text_file = dict()\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "        # Check whether file is in text format or not\n",
    "        if file.endswith(\".txt\"): \n",
    "            # get the index of the file\n",
    "            # ex. cv676_22202.txt --> 676\n",
    "            index = int(file.split('_')[0].split('cv')[1])\n",
    "            file_path = path + '/' + file\n",
    "            # call read text file function\n",
    "            text_file[index] = read_text_file(file_path)\n",
    "    return text_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b265b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_file = read_all_file(neg_path)\n",
    "pos_file = read_all_file(pos_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1110c9",
   "metadata": {},
   "source": [
    "### (b) Data Exploration and Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bd27bf",
   "metadata": {},
   "source": [
    "#### i. You can use binary encoding for the sentiments , i.e y = 1 for positive sentiments and y = −1 for negative sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09b785d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_data = pd.DataFrame.from_dict(neg_file, columns = ['content'] , orient = 'index')\n",
    "neg_data = neg_data.sort_index()\n",
    "neg_data['sentiment'] = -1\n",
    "\n",
    "pos_data = pd.DataFrame.from_dict(pos_file, columns = ['content'] , orient = 'index')\n",
    "pos_data = pos_data.sort_index()\n",
    "pos_data['sentiment'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23b4a00",
   "metadata": {},
   "source": [
    "#### ii. The data are pretty clean. Remove the punctuation and numbers from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "100ac8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(content):\n",
    "    # remove punctuation\n",
    "    new_content = content.translate(str.maketrans('', '', string.punctuation))\n",
    "    # remove number\n",
    "    new_content = ''.join([i for i in new_content if not i.isdigit()])\n",
    "    # remove space/strip\n",
    "    new_content = new_content.replace('\\n', '')\n",
    "    \n",
    "    return new_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e86db27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(neg_data)):\n",
    "    neg_data.loc[i, ['content']] = remove_punctuation(neg_data.iloc[i]['content'])\n",
    "for i in range(len(pos_data)):\n",
    "    pos_data.loc[i, ['content']] = remove_punctuation(pos_data.iloc[i]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c540a0",
   "metadata": {},
   "source": [
    "#### iii. The name of each text file starts with cv number. Use text files 0-699 in each class for training and 700-999 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b361006a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Training Dataset ==============\n",
      "Total data: 1400\n",
      "Negative: 700/1400\n",
      "Positive: 700/1400\n",
      "============== Testing Dataset ==============\n",
      "Total data: 600\n",
      "Negative: 300/600\n",
      "Positive: 300/600\n"
     ]
    }
   ],
   "source": [
    "neg_training = neg_data.iloc[:700]\n",
    "neg_testing = neg_data.iloc[700:]\n",
    "\n",
    "pos_training = pos_data.iloc[:700]\n",
    "pos_testing = pos_data.iloc[700:]\n",
    "\n",
    "# combine data and create training set and testing set\n",
    "training_data = pd.concat([neg_training, pos_training])\n",
    "training_data = training_data.reset_index(drop=True)\n",
    "testing_data = pd.concat([neg_testing, pos_testing])\n",
    "testing_data = testing_data.reset_index(drop=True)\n",
    "\n",
    "training_X = training_data[\"content\"]\n",
    "training_Y = training_data[\"sentiment\"]\n",
    "testing_X = testing_data[\"content\"]\n",
    "testing_Y = testing_data[\"sentiment\"]\n",
    "\n",
    "print(\"============== Training Dataset ==============\")\n",
    "print(\"Total data: \" + str(len(training_data)))\n",
    "print(\"Negative: \" + str(len(neg_training)) + \"/\" + str(len(training_data)))\n",
    "print(\"Positive: \" + str(len(pos_training)) + \"/\" + str(len(training_data)))\n",
    "print(\"============== Testing Dataset ==============\")\n",
    "print(\"Total data: \" + str(len(testing_data)))\n",
    "print(\"Negative: \" + str(len(neg_testing)) + \"/\" + str(len(testing_data)))\n",
    "print(\"Positive: \" + str(len(pos_testing)) + \"/\" + str(len(testing_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ba6d4b",
   "metadata": {},
   "source": [
    "#### iv. Count the number of unique words in the whole dataset (train + test) and print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1258aff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = dict()\n",
    "\n",
    "# count the word in training data\n",
    "for i in range(len(training_data)):\n",
    "    split_list = training_data.iloc[i]['content'].split(\" \")\n",
    "    for word in split_list:\n",
    "        if word not in word_list:\n",
    "            word_list[word] = 1\n",
    "        else:\n",
    "            word_list[word] += 1\n",
    "            \n",
    "# count the word in testing data\n",
    "for i in range(len(testing_data)):\n",
    "    split_list = testing_data.iloc[i]['content'].split(\" \")\n",
    "    for word in split_list:\n",
    "        if word not in word_list:\n",
    "            word_list[word] = 1\n",
    "        else:\n",
    "            word_list[word] += 1\n",
    "\n",
    "# remove useless key\n",
    "del word_list['']\n",
    "\n",
    "# sort the dict in descneding order\n",
    "sorted_word_list = dict(sorted(word_list.items(), key = operator.itemgetter(1), reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58a84d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46830"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e738d93c",
   "metadata": {},
   "source": [
    "#### v. Calculate the average review length and the standard deviation of review lengths. Report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb6b87bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_len = dict()\n",
    "for i in range(len(training_data)):\n",
    "    split_list = training_data.iloc[i]['content'].split(\" \")\n",
    "    # remove the \"\"\n",
    "    split_list = [value for value in split_list if value != \"\"]\n",
    "    review_len[i] = len(split_list)\n",
    "\n",
    "index_now = len(review_len)\n",
    "\n",
    "for i in range(len(testing_data)):\n",
    "    split_list = testing_data.iloc[i]['content'].split(\" \")\n",
    "    # remove the \"\"\n",
    "    split_list = [value for value in split_list if value != \"\"]\n",
    "    review_len[index_now + i] = len(split_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ac77bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Average Revies Length ==============\n",
      "999\n",
      "============== Standard Deviation of  Revies Length ==============\n",
      "577\n"
     ]
    }
   ],
   "source": [
    "print(\"============== Average Revies Length ==============\")\n",
    "print(int(sum(review_len) / len(review_len)))\n",
    "print(\"============== Standard Deviation of  Revies Length ==============\")\n",
    "print(int(statistics.pstdev(review_len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7e8ba9",
   "metadata": {},
   "source": [
    "#### vi. Plot the histogram of review lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2156f5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8ff680d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Histogram of Review Lengths')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(review_len.values(), bins = 10)  # density=False would make counts\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Count')\n",
    "plt.title('Histogram of Review Lengths')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34df8ed3",
   "metadata": {},
   "source": [
    "#### vii. To represent each text (= data point), there are many ways. In NLP/Deep Learning terminology, this task is called tokenization. It is common to represent text using popularity/ rank of words in text. The most common word in the text will be represented as 1, the second most common word will be represented as 2, etc. Tokenize each text document using this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c0daa82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49cbe750",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \"\"\n",
    "for i in range(len(training_data)):\n",
    "    all_text = all_text + \" \" + training_data.iloc[i]['content']\n",
    "for i in range(len(testing_data)):\n",
    "    all_text = all_text + \" \" + testing_data.iloc[i]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "163b77c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization_list = list() # store all data\n",
    "training_tokenization = list()\n",
    "testing_tokenization = list()\n",
    "\n",
    "t = Tokenizer()\n",
    "# fit to thw whole(traini+test) dataset\n",
    "t.fit_on_texts([all_text])\n",
    "\n",
    "for i in range(len(training_data)):\n",
    "    sequences = t.texts_to_sequences([training_data.iloc[i]['content']])\n",
    "    #tokenization_list.append(t.word_index)\n",
    "    tokenization_list.append(sequences[0])\n",
    "    training_tokenization.append(sequences[0])\n",
    "\n",
    "for i in range(len(testing_data)):\n",
    "    sequences = t.texts_to_sequences([testing_data.iloc[i]['content']])\n",
    "    tokenization_list.append(sequences[0])\n",
    "    testing_tokenization.append(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c487a249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenization_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d666b12",
   "metadata": {},
   "source": [
    "#### viii. Select a review length L that 70% of the reviews have a length below it. If you feel more adventurous, set the threshold to 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a73175b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization_review_len = dict()\n",
    "for i in range(len(tokenization_list)):\n",
    "    tokenization_review_len[i] = len(tokenization_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6184275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Length Threshold(70% of the reviews have a length below it): 737\n"
     ]
    }
   ],
   "source": [
    "# get the threshold of the length\n",
    "review_len_sorted = list({k: v for k, v in sorted(tokenization_review_len.items(), key = lambda item: item[1])}.values())\n",
    "top70 = round(len(tokenization_review_len)*0.7)\n",
    "filter_len = review_len_sorted[top70-1]\n",
    "print(\"Review Length Threshold(70% of the reviews have a length below it): \" + str(filter_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfccc24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def below_or_above(data, filter_len):\n",
    "    blw = list()\n",
    "    abv= list()\n",
    "    for i in range(len(data)):\n",
    "        if len(data[i]) <= filter_len:\n",
    "            blw.append(data[i])\n",
    "        else: \n",
    "            abv.append(data[i])\n",
    "    return blw, abv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b422a2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1404 reviews with length below the threshold.\n",
      "There are 596 reviews with length above the threshold.\n"
     ]
    }
   ],
   "source": [
    "#below_threshold = list()\n",
    "#above_threshold = list()\n",
    "training_blw = list()\n",
    "training_abv = list()\n",
    "testing_blw = list()\n",
    "testing_abv = list()\n",
    "\n",
    "training_blw, training_abv = below_or_above(training_tokenization, filter_len)\n",
    "testing_blw, testing_abv = below_or_above(testing_tokenization, filter_len)\n",
    "  \n",
    "print(\"There are \" + str(len(training_blw) + len(testing_blw)) + \" reviews with length below the threshold.\")\n",
    "print(\"There are \" + str(len(training_abv) + len(testing_abv))  + \" reviews with length above the threshold.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e944333",
   "metadata": {},
   "source": [
    "#### ix. Truncate reviews longer than L words and zero-pad reviews shorter than L so that all texts (= data points) are of length L."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c2c823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_training_X = keras.preprocessing.sequence.pad_sequences(training_tokenization, maxlen = filter_len)\n",
    "new_testing_X = keras.preprocessing.sequence.pad_sequences(testing_tokenization, maxlen = filter_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85ed069e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After modifying, there are 2000/2000 reviews with length of 737 now.\n"
     ]
    }
   ],
   "source": [
    "# check all reviews have the same value now\n",
    "count_T = 0\n",
    "count_F = 0\n",
    "for data in new_training_X:\n",
    "    if len(data)== filter_len:\n",
    "        count_T += 1\n",
    "    else:\n",
    "        count_F += 1\n",
    "for data in new_testing_X:\n",
    "    if len(data)== filter_len:\n",
    "        count_T += 1\n",
    "    else:\n",
    "        count_F += 1\n",
    "print(\"After modifying, there are \" + str(count_T) + \"/\" + str(len(new_training_X) + len(new_testing_X)) +  \" reviews with length of \" + str(filter_len) + \" now.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b844330",
   "metadata": {},
   "source": [
    "### (c) Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863b31d8",
   "metadata": {},
   "source": [
    "#### i. One can use tokenized text as inputs to a deep neural network. However, a recent breakthrough in NLP suggests that more sophisticated representations of text yield better results. These sophisticated representations are called word embeddings. “Word embedding is a term used for representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning.”4. Most deep learning modules (including Keras) provide a convenient way to convert positive integer rep- resentations of words into a word embedding by an “Embedding layer.” The layer accepts arguments that define the mapping of words into embeddings, including the maximum number of expected words also called the vocabulary size (e.g. the largest integer value). The layer also allows you to specify the dimension for each word vector, called the “output dimension.” We would like to use a word embedding layer for this project. Assume that we are inter- ested in the top 5,000 words. This means that in each integer sequence that represents each document, we set to zero those integers that represent words that are not among the top 5,000 words in the document.5 If you feel more adventurous, use all the words that appear in this corpus. Choose the length of the embedding vector for each word to be 32. Hence, each document is represented as a 32 × 500 matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe27dd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Flatten, Dense, Dropout, Conv1D, pooling, LSTM\n",
    "from keras.layers.pooling import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14ee7067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new datalist that contains only top500 words\n",
    "def filter_top5000(data):\n",
    "    top5000 = list()\n",
    "    for i in range(len(data)):\n",
    "        temp_list = list()\n",
    "        for j in data[i]:\n",
    "            if j>5000:\n",
    "                temp_list.append(0)\n",
    "            else:\n",
    "                temp_list.append(j)\n",
    "        top5000.append(temp_list)\n",
    "    return top5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac676ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_training_X = filter_top5000(new_training_X)\n",
    "top_testing_X = filter_top5000(new_testing_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62191568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top words -> size of the vocabulary or the total number of unique words in a corpus\n",
    "top_words = 5001\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(top_words, 32, input_length = filter_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a280712e",
   "metadata": {},
   "source": [
    "#### ii. Flatten the matrix of each document to a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de281c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.add(Flatten())\n",
    "#model1.add(Dense(1, activation='sigmoid'))\n",
    "#model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "#print(model1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad1f643",
   "metadata": {},
   "source": [
    "### (d) Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5286848",
   "metadata": {},
   "source": [
    "#### i. Train a MLP with three (dense) hidden layers each of which has 50 ReLUs and one output layer with a single sigmoid neuron. Use a dropout rate of 20% for the first layer and 50% for the other layers. Use ADAM optimizer and binary cross entropy loss (which is equivalent to having a softmax in the output). To avoid overfitting, just set the number of epochs as 2. Use a batch size of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "585d7dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE HOT ENCODING\n",
    "new_training_Y = list()\n",
    "new_testing_Y = list()\n",
    "\n",
    "for i in training_Y:\n",
    "    if i == 1:\n",
    "        new_training_Y.append(1)\n",
    "    else:\n",
    "        new_training_Y.append(0)\n",
    "        \n",
    "for i in testing_Y:\n",
    "    if i == 1:\n",
    "        new_testing_Y.append(1)\n",
    "    else:\n",
    "        new_testing_Y.append(0)\n",
    "        \n",
    "new_training_Y = pd.Series(new_training_Y)\n",
    "new_testing_Y = pd.Series(new_testing_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "460d0761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 737, 32)           160032    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 23584)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                1179250   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,344,433\n",
      "Trainable params: 1,344,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# add three hidden layers\n",
    "model1.add(Dense(units = 50, activation='relu')) \n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(Dense(units = 50, activation='relu')) \n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Dense(units = 50, activation='relu')) \n",
    "model1.add(Dropout(0.5))\n",
    "# add output layer\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c8169da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Charlie/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1400 samples, validate on 600 samples\n",
      "Epoch 1/2\n",
      "1400/1400 [==============================] - 4s 3ms/step - loss: 0.6951 - acc: 0.5364 - val_loss: 0.6854 - val_acc: 0.5550\n",
      "Epoch 2/2\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 0.6253 - acc: 0.6271 - val_loss: 0.6552 - val_acc: 0.6100\n",
      "Training Accuracy: 92.14285612106323%\n",
      "Testing Accuracy: 61.000001430511475%\n"
     ]
    }
   ],
   "source": [
    "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "top_training_X = np.array(top_training_X)\n",
    "top_testing_X = np.array(top_testing_X)\n",
    "model1.fit(top_training_X, new_training_Y, validation_data=(top_testing_X, new_testing_Y), epochs=2, batch_size=10)\n",
    "\n",
    "train_loss, train_accuracy = model1.evaluate(top_training_X, new_training_Y, verbose=0)\n",
    "test_loss, test_accuracy = model1.evaluate(top_testing_X, new_testing_Y, verbose=0)\n",
    "print(\"Training Accuracy: \" + str(train_accuracy*100) + \"%\")\n",
    "print(\"Testing Accuracy: \" + str(test_accuracy*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944e062d",
   "metadata": {},
   "source": [
    "### (e) One-Dimensional Convolutional Neural Network:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfb2ed6",
   "metadata": {},
   "source": [
    "Although CNNs are mainly used for image data, they can also be applied to text data, as text also has adjacency information. Keras supports one-dimensional convolutions and pooling by the Conv1D and MaxPooling1D classes respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bf42a5",
   "metadata": {},
   "source": [
    "#### i. After the embedding layer, insert a Conv1D layer. This convolutional layer has 32 feature maps , and each of the 32 kernels has size 3, i.e. reads embedded word representations 3 vector elements of the word embedding at a time. The convolutional layer is followed by a 1D max pooling layer with a length and stride of 2 that halves the size of the feature maps from the convolutional layer. The rest of the network is the same as the neural network above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54e4106f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 737, 32)           160032    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 735, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 367, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 11744)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                587250    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 755,537\n",
      "Trainable params: 755,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(top_words, 32, input_length = filter_len))\n",
    "model2.add(Conv1D(filters = 32, kernel_size = 3))\n",
    "model2.add(MaxPooling1D(pool_size = 2, strides = 2))\n",
    "model2.add(Flatten())\n",
    "# add three hidden layers\n",
    "model2.add(Dense(units = 50, activation='relu')) \n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(units = 50, activation='relu')) \n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(units = 50, activation='relu')) \n",
    "model2.add(Dropout(0.5))\n",
    "# add output layer\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "af728d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Charlie/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1400 samples, validate on 600 samples\n",
      "Epoch 1/2\n",
      "1400/1400 [==============================] - 5s 3ms/step - loss: 0.6981 - acc: 0.5086 - val_loss: 0.6914 - val_acc: 0.5383\n",
      "Epoch 2/2\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 0.6853 - acc: 0.5393 - val_loss: 0.6821 - val_acc: 0.5550\n",
      "Training Accuracy: 72.85714149475098%\n",
      "Testing Accuracy: 55.50000071525574%\n"
     ]
    }
   ],
   "source": [
    "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model2.fit(top_training_X, new_training_Y, validation_data=(top_testing_X, new_testing_Y), epochs=2, batch_size=10)\n",
    "\n",
    "train_loss_2, train_accuracy_2 = model2.evaluate(top_training_X, new_training_Y, verbose=0)\n",
    "test_loss_2, test_accuracy_2 = model2.evaluate(top_testing_X, new_testing_Y, verbose=0)\n",
    "print(\"Training Accuracy: \" + str(train_accuracy_2*100) + \"%\")\n",
    "print(\"Testing Accuracy: \" + str(test_accuracy_2*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742f83a4",
   "metadata": {},
   "source": [
    "### (f) Long Short-Term Memory Recurrent Neural Network:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9da745",
   "metadata": {},
   "source": [
    "The structure of the LSTM we are going to use is shown in the following figure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f517ea",
   "metadata": {},
   "source": [
    "#### i. Each word is represented to LSTM as a vector of 32 elements and the LSTM is followed by a dense layer of 256 ReLUs. Use a dropout rate of 0.2 for both LSTM and the dense layer. Train the model using 10-50 epochs and batch size of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b1c50174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 737, 32)           160032    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 256)               8448      \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 177,057\n",
      "Trainable params: 177,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Embedding(top_words, 32, input_length = filter_len))\n",
    "model3.add(LSTM(units = 32))\n",
    "model3.add(Dropout(0.2))\n",
    "model3.add(Dense(units = 256, activation = 'relu')) \n",
    "model3.add(Dropout(0.2))\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f89803e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Charlie/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1400 samples, validate on 600 samples\n",
      "Epoch 1/15\n",
      "1400/1400 [==============================] - 154s 110ms/step - loss: 0.6928 - acc: 0.5136 - val_loss: 0.6875 - val_acc: 0.5650\n",
      "Epoch 2/15\n",
      "1400/1400 [==============================] - 144s 103ms/step - loss: 0.5373 - acc: 0.7543 - val_loss: 0.6058 - val_acc: 0.6833\n",
      "Epoch 3/15\n",
      "1400/1400 [==============================] - 180s 128ms/step - loss: 0.2461 - acc: 0.9000 - val_loss: 0.7375 - val_acc: 0.7100\n",
      "Epoch 4/15\n",
      "1400/1400 [==============================] - 184s 132ms/step - loss: 0.1076 - acc: 0.9579 - val_loss: 0.8555 - val_acc: 0.7467\n",
      "Epoch 5/15\n",
      "1400/1400 [==============================] - 144s 103ms/step - loss: 0.0539 - acc: 0.9807 - val_loss: 1.0412 - val_acc: 0.7150\n",
      "Epoch 6/15\n",
      "1400/1400 [==============================] - 159s 114ms/step - loss: 0.0063 - acc: 1.0000 - val_loss: 1.2771 - val_acc: 0.7267\n",
      "Epoch 7/15\n",
      "1400/1400 [==============================] - 150s 107ms/step - loss: 9.8602e-04 - acc: 1.0000 - val_loss: 1.4549 - val_acc: 0.7267\n",
      "Epoch 8/15\n",
      "1400/1400 [==============================] - 147s 105ms/step - loss: 3.5390e-04 - acc: 1.0000 - val_loss: 1.5305 - val_acc: 0.7267\n",
      "Epoch 9/15\n",
      "1400/1400 [==============================] - 148s 106ms/step - loss: 2.3929e-04 - acc: 1.0000 - val_loss: 1.5997 - val_acc: 0.7283\n",
      "Epoch 10/15\n",
      "1400/1400 [==============================] - 146s 104ms/step - loss: 1.6197e-04 - acc: 1.0000 - val_loss: 1.6696 - val_acc: 0.7233\n",
      "Epoch 11/15\n",
      "1400/1400 [==============================] - 148s 106ms/step - loss: 1.3348e-04 - acc: 1.0000 - val_loss: 1.7162 - val_acc: 0.7200\n",
      "Epoch 12/15\n",
      "1400/1400 [==============================] - 146s 104ms/step - loss: 8.7290e-05 - acc: 1.0000 - val_loss: 1.7615 - val_acc: 0.7233\n",
      "Epoch 13/15\n",
      "1400/1400 [==============================] - 141s 100ms/step - loss: 7.4050e-05 - acc: 1.0000 - val_loss: 1.7879 - val_acc: 0.7267\n",
      "Epoch 14/15\n",
      "1400/1400 [==============================] - 145s 103ms/step - loss: 7.0332e-05 - acc: 1.0000 - val_loss: 1.8286 - val_acc: 0.7267\n",
      "Epoch 15/15\n",
      "1400/1400 [==============================] - 164s 117ms/step - loss: 5.6868e-05 - acc: 1.0000 - val_loss: 1.8697 - val_acc: 0.7233\n",
      "Training Accuracy: 100.0%\n",
      "Testing Accuracy: 72.33333587646484%\n"
     ]
    }
   ],
   "source": [
    "model3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model3.fit(top_training_X, new_training_Y, validation_data=(top_testing_X, new_testing_Y), epochs=15, batch_size=10)\n",
    "\n",
    "train_loss_3, train_accuracy_3 = model3.evaluate(top_training_X, new_training_Y, verbose=0)\n",
    "test_loss_3, test_accuracy_3 = model3.evaluate(top_testing_X, new_testing_Y, verbose=0)\n",
    "print(\"Training Accuracy: \" + str(train_accuracy_3*100) + \"%\")\n",
    "print(\"Testing Accuracy: \" + str(test_accuracy_3*100) + \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
